{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1avKW5S3NjJjuBZZyGWK113aJT4Eomd2T",
      "authorship_tag": "ABX9TyPuYPng5oSfFAf+recgpezZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumarpriyanshu09/Gemma-3-4B-Fine-tuning-with-Unsloth/blob/main/Finetuning_Gemma3_(4B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUu4j1pXN5-T"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name =\"unsloth/gemma-3-4b-it\",\n",
        "    max_seq_length= 2048,\n",
        "    load_in_4bit=True,\n",
        "    load_in_8bit=False,\n",
        "    full_finetuning=False,\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nIiVrGJ-Oc9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetuning_vision_layers = False,\n",
        "    finetuning_language_layers = True,\n",
        "    finetuning_attention_modules = True,\n",
        "    finetuning_mlp_modules = True,\n",
        "\n",
        "    r = 8,\n",
        "    lora_alpha =8,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3047,\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BR34V57dT7-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")"
      ],
      "metadata": {
        "id": "lDCFM9I8TPU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
      ],
      "metadata": {
        "id": "fo0ZjuVEVGRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import standardize_data_formats\n",
        "dataset = standardize_data_formats(dataset)"
      ],
      "metadata": {
        "id": "vE1fiKvfVXIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[96]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zHg5dHRNXC1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_fuc(examples):\n",
        "  convos = examples[\"conversations\"]\n",
        "  texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
        "  return { \"text\" : texts , }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_fuc, batched = True)"
      ],
      "metadata": {
        "id": "P0I3zbf2XKm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[96][\"text\"]"
      ],
      "metadata": {
        "id": "oC7UuIXdYcXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer =  SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        #num_train_epochs = 1,\n",
        "        max_steps = 32,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps=1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "C0g84u-Rab3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import  train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part= \"<start_of_turn>model\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "fbV2ryg7d96p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(trainer.train_dataset[96][\"input_ids\"])"
      ],
      "metadata": {
        "id": "HtDQgqrYfrWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[96][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
      ],
      "metadata": {
        "id": "NNRyk1pogckg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_stats =  torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max Memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "FIQpihgigtXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "AgoTIzwpiGTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "vJh9Phy5iUjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\n",
        "        \"type\" : \"text\",\n",
        "        \"text\" : \"Continue the sequence: 1, 1, 2, 3, 5, 8,\",\n",
        "    }]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "outputs = model.generate(\n",
        "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        ")\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WWWa_DjKl49W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"type\": \"text\", \"text\": \"why is the sky blue?\"}]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt =True,\n",
        ")\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer([text], return_tensors = 'pt').to(\"cuda\"),\n",
        "    max_new_tokens = 64,\n",
        "    temperature = 1.0,\n",
        "    top_p = 0.95,\n",
        "    top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_promot = True),\n",
        ")"
      ],
      "metadata": {
        "id": "yw9xIwEjqQCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"gemma-3-lora\")\n",
        "tokenizer.save_pretrained(\"gemma-3-lora\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "e5Nsa6kurjDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)"
      ],
      "metadata": {
        "id": "O8j2AMxEewui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(\n",
        "    \"gemma-3-finetune\",\n",
        "    quantization_type=\"Q8_0\"\n",
        ")"
      ],
      "metadata": {
        "id": "2pHPJbVXecgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "  from unsloth import FastModel\n",
        "  model,tokenzizer = FastModel.from_pretrained(\n",
        "      model_name = \"path/to/saved/model\",\n",
        "      load_in_4bit = True,\n",
        "  )\n",
        "\n",
        "  messages = [{\n",
        "      \"role\" : \"user\",\n",
        "      \"content\": [{\"type\": \"text\", \"text\": \"why is the Gemma-3?\"}]\n",
        "  }]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer([text], return_tensors = 'pt').to(\"cuda\"),\n",
        "    max_new_tokens = 64,\n",
        "    temperature = 1.0,\n",
        "    top_p = 0.95,\n",
        "    top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ],
      "metadata": {
        "id": "HKR_uwsmszxY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}